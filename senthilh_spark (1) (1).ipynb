{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e17eb14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f995c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/03 21:50:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/03 21:50:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeatherDataAnalysis\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b6acae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2022, Station: 72429793812, Record Count: 365\n",
      "Year: 2022, Station: 99495199999, Record Count: 259\n",
      "Year: 2024, Station: 72429793812, Record Count: 296\n",
      "Year: 2024, Station: 99495199999, Record Count: 133\n",
      "Year: 2023, Station: 72429793812, Record Count: 365\n",
      "Year: 2023, Station: 99495199999, Record Count: 276\n",
      "Year: 2015, Station: 72429793812, Record Count: 365\n",
      "Year: 2015, Station: 99495199999, Record Count: 355\n",
      "Year: 2017, Station: 72429793812, Record Count: 365\n",
      "Year: 2017, Station: 99495199999, Record Count: 283\n",
      "Year: 2019, Station: 72429793812, Record Count: 365\n",
      "Year: 2019, Station: 99495199999, Record Count: 345\n",
      "Year: 2021, Station: 72429793812, Record Count: 365\n",
      "Year: 2021, Station: 99495199999, Record Count: 104\n",
      "Year: 2020, Station: 72429793812, Record Count: 366\n",
      "Year: 2020, Station: 99495199999, Record Count: 365\n",
      "Year: 2018, Station: 72429793812, Record Count: 365\n",
      "Year: 2018, Station: 99495199999, Record Count: 363\n",
      "Year: 2016, Station: 72429793812, Record Count: 366\n",
      "Year: 2016, Station: 99495199999, Record Count: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import year, to_date\n",
    "\n",
    "# Define the base path for the data directory\n",
    "base_path = os.path.expanduser('~/Documents/Project 4/WeatherData') \n",
    "stations = ['72429793812', '99495199999']\n",
    "\n",
    "# Create an empty list to store results\n",
    "data_counts = []\n",
    "\n",
    "# Iterate through year directories in the base path\n",
    "for year_dir in os.listdir(base_path):\n",
    "    if year_dir.isdigit() and int(year_dir) in range(2015, 2025):  # Check if the directory name is a valid year\n",
    "        year_path = os.path.join(base_path, year_dir)\n",
    "        \n",
    "        # Process CSV files in the year directory\n",
    "        for station in stations:\n",
    "            file_path = os.path.join(year_path, f\"{station}.csv\")\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    # Read the CSV file with inferSchema\n",
    "                    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "                    \n",
    "                    # Get count and year from the first date in the dataset\n",
    "                    count = df.count()\n",
    "                    yr = df.select(year(to_date(\"DATE\")).alias(\"year\")).first()[0]  # Extract year from date column\n",
    "\n",
    "                    # Add to results\n",
    "                    data_counts.append((yr, station, count))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                    data_counts.append((year_dir, station, 0))  # Append zero count on error\n",
    "            else:\n",
    "                data_counts.append((year_dir, station, 0))  # Append zero count if file does not exist\n",
    "\n",
    "# Print the results\n",
    "for record in data_counts:\n",
    "    year, station, count = record\n",
    "    print(f\"Year: {year}, Station: {station}, Record Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09882c67-c25d-4337-a5c6-70d114d3b901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------------------------------------+----------+------+\n",
      "|YEAR|STATION    |NAME                                            |DATE      |MAX   |\n",
      "+----+-----------+------------------------------------------------+----------+------+\n",
      "|2015|72429793812|CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US|2015-06-12|91.9  |\n",
      "|2016|72429793812|CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US|2016-07-24|93.9  |\n",
      "|2017|99495199999|SEBASTIAN INLET STATE PARK, FL US               |2017-02-22|9999.9|\n",
      "|2018|72429793812|CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US|2018-07-04|96.1  |\n",
      "|2019|72429793812|CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US|2019-09-30|95.0  |\n",
      "|2020|72429793812|CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US|2020-07-05|93.9  |\n",
      "|2021|72429793812|CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US|2021-08-12|95.0  |\n",
      "|2022|72429793812|CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US|2022-12-23|9999.9|\n",
      "|2023|72429793812|CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US|2023-08-23|96.1  |\n",
      "|2024|72429793812|CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US|2024-08-30|100.9 |\n",
      "+----+-----------+------------------------------------------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import year, col, row_number\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for yr in years:\n",
    "    for station in stations:\n",
    "        file_path = f\"{base_path}/{yr}/{station}.csv\"\n",
    "        if os.path.exists(file_path):\n",
    "            df = spark.read.csv(file_path, header=True)\n",
    "            dfs.append(df)\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "all_data = reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "all_data = all_data.withColumn('YEAR', year(col('DATE'))) \\\n",
    "                   .withColumn('MAX', col('MAX').cast('float')) \\\n",
    "                   .withColumn('MIN', col('MIN').cast('float')) \\\n",
    "                   .filter((col('MAX') != 9999.9) & (col('MAX').isNotNull()))\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec = Window.partitionBy('YEAR').orderBy(col('MAX').desc())\n",
    "\n",
    "hottest_days = all_data.withColumn('rank', row_number().over(windowSpec)) \\\n",
    "                       .filter(col('rank') == 1) \\\n",
    "                       .select('YEAR', 'STATION', 'NAME', 'DATE', 'MAX')\n",
    "\n",
    "hottest_days.show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c8f03f5-ee57-4e15-a347-55ff03059efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------------------+----------+-------------------+\n",
      "|STATION    |NAME                                            |DATE      |Minimum_Temperature|\n",
      "+-----------+------------------------------------------------+----------+-------------------+\n",
      "|72429793812|CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US|2015-03-06|3.2                |\n",
      "+-----------+------------------------------------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, col\n",
    "\n",
    "# Filter data for March and clean the MIN column\n",
    "march_weather_data = all_data.filter(month(col('DATE')) == 3) \\\n",
    "                             .withColumn('Minimum_Temperature', col('MIN').cast('float')) \\\n",
    "                             .filter((col('Minimum_Temperature') != 9999.9) & (col('Minimum_Temperature').isNotNull()))\n",
    "\n",
    "# Find the coldest day in March\n",
    "coldest_record = march_weather_data.orderBy(col('Minimum_Temperature').asc()).limit(1) \\\n",
    "                                    .select('STATION', 'NAME', 'DATE', 'Minimum_Temperature')\n",
    "\n",
    "# Display the result\n",
    "coldest_record.show(1, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b914286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------------------+----+------------------+\n",
      "|STATION    |NAME                                            |YEAR|Total_PRCP        |\n",
      "+-----------+------------------------------------------------+----+------------------+\n",
      "|72429793812|CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US|2024|1636.1299657933414|\n",
      "|99495199999|SEBASTIAN INLET STATE PARK, FL US               |2015|0.0               |\n",
      "+-----------+------------------------------------------------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "valid_precipitation = all_data.withColumn('PRCP', col('PRCP').cast('float')) \\\n",
    "                              .filter((col('PRCP').isNotNull()) & \n",
    "                                      (col('PRCP') != 99.99) & \n",
    "                                      (col('PRCP') != 999.9))\n",
    "\n",
    "precipitation_data = valid_precipitation.groupBy('STATION', 'NAME', 'YEAR') \\\n",
    "                                        .agg(F.sum('PRCP').alias('Total_PRCP'))\n",
    "\n",
    "windowSpec = Window.partitionBy('STATION').orderBy(col('Total_PRCP').desc())\n",
    "\n",
    "max_precipitation = precipitation_data.withColumn('rank', row_number().over(windowSpec)) \\\n",
    "                                      .filter(col('rank') == 1) \\\n",
    "                                      .select('STATION', 'NAME', 'YEAR', 'Total_PRCP')\n",
    "\n",
    "max_precipitation.show(2, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "691a2484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values for Cincinnati (GUST): 39.53%\n",
      "Percentage of missing values for Florida (GUST): 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to calculate missing percentage for a given DataFrame\n",
    "def calculate_missing_percentage(data, station_name):\n",
    "    total_count = data.count()\n",
    "    missing_count = data.filter(col(\"GUST\").isNull() | (col(\"GUST\") == missing_value)).count()\n",
    "    missing_percentage = (missing_count / total_count) * 100 if total_count > 0 else 0\n",
    "    print(f\"Percentage of missing values for {station_name} (GUST): {missing_percentage:.2f}%\")\n",
    "\n",
    "# Calculate and print missing percentage for Cincinnati\n",
    "if cincinnati_data_2024:\n",
    "    calculate_missing_percentage(cincinnati_data_2024, \"Cincinnati\")\n",
    "\n",
    "# Calculate and print missing percentage for Florida\n",
    "if florida_data_2024:\n",
    "    calculate_missing_percentage(florida_data_2024, \"Florida\")\n",
    "\n",
    "# Additional analysis for all_data if required (assuming 'all_data' is defined)\n",
    "data_2024 = all_data.filter(col('YEAR') == 2024).withColumn('GUST', col('GUST').cast('float'))\n",
    "\n",
    "# Calculate total counts per station\n",
    "total_counts = data_2024.groupBy('STATION').agg(count('*').alias('Total_Count'))\n",
    "\n",
    "# Calculate missing counts where GUST is null or equals 999.9\n",
    "missing_counts = data_2024.filter((col('GUST').isNull()) | (col('GUST') == missing_value)) \\\n",
    "                              .groupBy('STATION') \\\n",
    "                              .agg(count('*').alias('Missing_Count'))\n",
    "\n",
    "# Join and calculate missing percentage for all data\n",
    "joined_counts = total_counts.join(missing_counts, on='STATION', how='left') \\\n",
    "                            .fillna(0, subset=['Missing_Count'])\n",
    "\n",
    "percentage_missing = joined_counts.withColumn('Missing_Percentage', \n",
    "                                              (col('Missing_Count') / col('Total_Count')) * 100) \\\n",
    "                                  .select('STATION', 'Missing_Percentage')\n",
    "\n",
    "# Display the results for all_data\n",
    "percentage_missing.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6aa775d8-f1c0-455a-a120-2903e2198099",
   "metadata": {},
   "outputs": [],
   "source": [
    "cincinnati_2020 = all_data.filter((col('STATION') == '72429793812') & (col('YEAR') == 2020)) \\\n",
    "                          .withColumn('TEMP', col('TEMP').cast('float')) \\\n",
    "                          .withColumn('MONTH', month(col('DATE'))) \\\n",
    "                          .filter((col('TEMP') != 9999.9) & (col('TEMP').isNotNull()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4241a5f-e965-47d2-9e6d-0191e2cf3ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, expr\n",
    "\n",
    "stats = cincinnati_2020.groupBy('MONTH') \\\n",
    "                       .agg(mean('TEMP').alias('Mean_TEMP'),\n",
    "                            expr('percentile_approx(TEMP, 0.5)').alias('Median_TEMP'),\n",
    "                            stddev('TEMP').alias('StdDev_TEMP')) \\\n",
    "                       .orderBy('MONTH')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7107a46d-b942-44e1-959b-a10c157e53ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------+------------------+---------+\n",
      "|MONTH|Mean_TEMP         |Median_TEMP|StdDev_TEMP       |Mode_TEMP|\n",
      "+-----+------------------+-----------+------------------+---------+\n",
      "|1    |37.945161081129505|37.7       |8.345810838316384 |24.7     |\n",
      "|2    |36.58965525133856 |36.0       |7.901597947537755 |25.9     |\n",
      "|3    |49.0741934007214  |47.8       |8.77940669347644  |39.6     |\n",
      "|4    |51.77999992370606 |51.0       |7.3131621276074465|49.4     |\n",
      "|5    |60.89032290058751 |63.7       |9.314768319579512 |73.9     |\n",
      "|6    |72.54666570027669 |73.7       |4.8999458590264515|70.7     |\n",
      "|7    |77.6000001968876  |77.9       |2.337947626620972 |78.4     |\n",
      "|8    |73.34516143798828 |73.7       |3.4878690606063563|78.3     |\n",
      "|9    |66.09999961853028 |65.8       |7.118261579669542 |74.5     |\n",
      "|10   |55.19354851015152 |54.0       |6.7286914818367975|52.2     |\n",
      "|11   |48.00333340962728 |47.7       |6.825938707865554 |47.7     |\n",
      "|12   |35.99354830095845 |35.2       |6.6427872766495755|32.1     |\n",
      "+-----+------------------+-----------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "mode_temp = cincinnati_2020.groupBy('MONTH', 'TEMP') \\\n",
    "                           .agg(count('TEMP').alias('Count')) \\\n",
    "                           .withColumn('rn', row_number().over(Window.partitionBy('MONTH').orderBy(col('Count').desc()))) \\\n",
    "                           .filter(col('rn') == 1) \\\n",
    "                           .select('MONTH', col('TEMP').alias('Mode_TEMP'))\n",
    "\n",
    "final_stats = stats.join(mode_temp, on='MONTH')\n",
    "\n",
    "final_stats.show(12, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e9460906-b539-4288-b754-87a5da5f9408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-------------------+\n",
      "|DATE      |TEMP|WDSP|Wind_Chill         |\n",
      "+----------+----+----+-------------------+\n",
      "|2017-01-07|10.5|7.0 |-0.4140156367932173|\n",
      "|2017-12-31|11.0|5.3 |2.0339764741541018 |\n",
      "|2017-12-27|13.0|5.8 |3.8206452986638073 |\n",
      "|2017-12-28|13.6|5.8 |4.533355513517824  |\n",
      "|2017-01-06|13.6|5.5 |4.868933492954463  |\n",
      "|2017-01-08|15.9|5.2 |7.929747979856229  |\n",
      "|2017-12-25|25.8|13.5|14.285112249501509 |\n",
      "|2017-12-30|21.6|5.3 |14.539211503699956 |\n",
      "|2017-01-05|22.2|5.8 |14.748862551376547 |\n",
      "|2017-12-26|23.3|6.2 |15.688977064714743 |\n",
      "+----------+----+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, pow\n",
    "\n",
    "# Filter the data for Cincinnati in 2017\n",
    "cincinnati_2017 = all_data.filter((col('STATION') == '72429793812') & (col('YEAR') == 2017))\n",
    "\n",
    "# Further filter the data based on the specified conditions\n",
    "filtered_data = cincinnati_2017.filter((col('TEMP').cast('float') < 50) & \n",
    "                                        (col('WDSP').cast('float') > 3) &\n",
    "                                        (col('TEMP') != 9999.9) & (col('WDSP') != 999.9))\n",
    "\n",
    "# Calculate Wind Chill\n",
    "wind_chill_data = filtered_data.withColumn('TEMP', col('TEMP').cast('float')) \\\n",
    "                               .withColumn('WDSP', col('WDSP').cast('float')) \\\n",
    "                               .withColumn('Wind_Chill', 35.74 + 0.6215 * col('TEMP') - 35.75 * pow(col('WDSP'), 0.16) + 0.4275 * col('TEMP') * pow(col('WDSP'), 0.16))\n",
    "\n",
    "# Get the 10 records with the lowest Wind Chill, including TEMP and WDSP\n",
    "lowest_wind_chill = wind_chill_data.orderBy(col('Wind_Chill').asc()) \\\n",
    "                                    .select('DATE', 'TEMP', 'WDSP', 'Wind_Chill') \\\n",
    "                                    .limit(10)\n",
    "\n",
    "# Display the results\n",
    "lowest_wind_chill.show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f72785c-94d0-4812-9da9-b184e1a5f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total days with extreme weather conditions in Florida: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lpad, col\n",
    "\n",
    "# Filter data for Florida station\n",
    "florida_station_data = all_data.filter(col('STATION') == '99495199999')\n",
    "\n",
    "# Identify extreme weather conditions\n",
    "weather_conditions = florida_station_data.filter(col('FRSHTT').isNotNull()) \\\n",
    "                                         .withColumn('FRSHTT', lpad(col('FRSHTT').cast('string'), 6, '0')) \\\n",
    "                                         .withColumn('IsExtreme', \n",
    "                                                     (col('FRSHTT').substr(1, 1) == '1') |  # Fog\n",
    "                                                     (col('FRSHTT').substr(2, 1) == '1') |  # Rain\n",
    "                                                     (col('FRSHTT').substr(3, 1) == '1') |  # Snow\n",
    "                                                     (col('FRSHTT').substr(4, 1) == '1') |  # Hail\n",
    "                                                     (col('FRSHTT').substr(5, 1) == '1') |  # Thunder\n",
    "                                                     (col('FRSHTT').substr(6, 1) == '1'))   # Tornado\n",
    "\n",
    "# Count the number of days with extreme weather\n",
    "extreme_weather_days = weather_conditions.filter(col('IsExtreme')).count()\n",
    "\n",
    "# Output the result\n",
    "print(f\"Total days with extreme weather conditions in Florida: {extreme_weather_days}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941b3ab-83ce-4862-9a94-d613778ebe9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "051cbf54-c55d-4cfc-aa43-0ead32d98d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|YEAR|MONTH|DAY|\n",
      "+----+-----+---+\n",
      "|2024|   11|  1|\n",
      "|2024|   11|  2|\n",
      "|2024|   11|  3|\n",
      "|2024|   11|  4|\n",
      "|2024|   11|  5|\n",
      "|2024|   11|  6|\n",
      "|2024|   11|  7|\n",
      "|2024|   11|  8|\n",
      "|2024|   11|  9|\n",
      "|2024|   11| 10|\n",
      "|2024|   11| 11|\n",
      "|2024|   11| 12|\n",
      "|2024|   11| 13|\n",
      "|2024|   11| 14|\n",
      "|2024|   11| 15|\n",
      "|2024|   11| 16|\n",
      "|2024|   11| 17|\n",
      "|2024|   11| 18|\n",
      "|2024|   11| 19|\n",
      "|2024|   11| 20|\n",
      "+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Generate prediction dates for November and December 2024\n",
    "november_days = [(2024, 11, day) for day in range(1, 31)]\n",
    "december_days = [(2024, 12, day) for day in range(1, 32)]\n",
    "all_prediction_dates = november_days + december_days\n",
    "\n",
    "# Create a DataFrame with the prediction dates\n",
    "prediction_dataframe = pd.DataFrame(all_prediction_dates, columns=[\"YEAR\", \"MONTH\", \"DAY\"])\n",
    "\n",
    "# Specify the path to save the CSV file\n",
    "csv_file_path = \"prediction_dates.csv\"  # Modify this path as needed\n",
    "prediction_dataframe.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Load the prediction dates from the CSV into a Spark DataFrame\n",
    "prediction_spark_data = spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "prediction_spark_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4ccce074-e789-4012-bb90-b669c824c9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|MONTH|   Max_Prediction|\n",
      "+-----+-----------------+\n",
      "|   12|54.13378106595995|\n",
      "|   11|66.55222876541171|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, dayofmonth, to_timestamp, col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WeatherDataAnalysis\").getOrCreate()\n",
    "\n",
    "# Define the base path for the data directory\n",
    "base_path = os.path.expanduser('~/Documents/Project 4/WeatherData') \n",
    "stations = ['72429793812']\n",
    "\n",
    "# Initialize variable to hold Cincinnati data\n",
    "cincinnati_combined_data = None\n",
    "\n",
    "# Iterate through year directories in the base path\n",
    "for year_dir in os.listdir(base_path):\n",
    "    if year_dir.isdigit() and int(year_dir) in range(2015, 2025):  # Check if the directory name is a valid year\n",
    "        year_path = os.path.join(base_path, year_dir)\n",
    "        \n",
    "        # Process CSV files in the year directory\n",
    "        for station in stations:\n",
    "            file_path = os.path.join(year_path, f\"{station}.csv\")\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    # Read the CSV file with header and infer schema\n",
    "                    yearly_data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "                    # Process the data to extract YEAR, MONTH, DAY, and cast MAX to float\n",
    "                    yearly_data = yearly_data.withColumn(\"YEAR\", year(to_timestamp(col(\"DATE\")))) \\\n",
    "                                             .withColumn(\"MONTH\", month(to_timestamp(col(\"DATE\")))) \\\n",
    "                                             .withColumn(\"DAY\", dayofmonth(to_timestamp(col(\"DATE\")))) \\\n",
    "                                             .withColumn(\"MAX\", col(\"MAX\").cast(\"float\")) \\\n",
    "                                             .filter(col(\"MONTH\").isin(11, 12)) \\\n",
    "                                             .filter((col(\"MAX\") < 9999.9) & (col(\"MAX\").isNotNull()))\n",
    "\n",
    "                    # Combine the yearly data into a single DataFrame\n",
    "                    cincinnati_combined_data = yearly_data if cincinnati_combined_data is None else cincinnati_combined_data.union(yearly_data)\n",
    "                except Exception as error:\n",
    "                    print(f\"Error loading {file_path}: {error}\")\n",
    "            else:\n",
    "                print(f\"File does not exist: {file_path}\")\n",
    "\n",
    "# Create a feature vector from YEAR, MONTH, and DAY\n",
    "vector_assembler = VectorAssembler(inputCols=[\"YEAR\", \"MONTH\", \"DAY\"], outputCol=\"features\")\n",
    "cincinnati_combined_data = vector_assembler.transform(cincinnati_combined_data).select(\"features\", \"MAX\")\n",
    "\n",
    "# Set up a Random Forest Regressor model in a pipeline\n",
    "random_forest_model = RandomForestRegressor(featuresCol=\"features\", labelCol=\"MAX\")\n",
    "model_pipeline = Pipeline(stages=[random_forest_model])\n",
    "\n",
    "# Fit the model to the combined data\n",
    "trained_model = model_pipeline.fit(cincinnati_combined_data)\n",
    "\n",
    "# Load prediction data from the CSV file (make sure this path is correct)\n",
    "prediction_data = spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "\n",
    "# Cast YEAR, MONTH, and DAY to integer types in the prediction DataFrame\n",
    "prediction_data = prediction_data.withColumn(\"YEAR\", col(\"YEAR\").cast(\"int\")) \\\n",
    "                                  .withColumn(\"MONTH\", col(\"MONTH\").cast(\"int\")) \\\n",
    "                                  .withColumn(\"DAY\", col(\"DAY\").cast(\"int\"))\n",
    "\n",
    "# Transform the prediction data to create feature vectors\n",
    "prediction_data = vector_assembler.transform(prediction_data)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predicted_results = trained_model.transform(prediction_data)\n",
    "\n",
    "# Aggregate the maximum predictions by MONTH\n",
    "max_monthly_predictions = predicted_results.groupBy(\"MONTH\").agg(F.max(\"prediction\").alias(\"Max_Prediction\"))\n",
    "\n",
    "# Show the results\n",
    "max_monthly_predictions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbccc6-d794-47b4-9ef9-ba3c430969cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
